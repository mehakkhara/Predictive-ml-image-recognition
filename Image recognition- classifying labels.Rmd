---
title: "Applied Data Science"
author: "Mehak Khara, Xueyan Zou, Chen(Cici) Chen"
date: "3/13/2019"
output:
  prettydoc::html_pretty:
  theme: cayman
highlight: github
---

```{r setup, include=FALSE}
set.seed(72)
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55))
```

```{r libraries, echo = FALSE}
library(data.table)
library(DT)
library(nnet)
library(glmnet)
library(class)
library(randomForest)
library(gbm)
library(rpart)
```


```{r source_files, echo=FALSE}
train.file <- fread((input = '.../data/MNIST-fashion trainingnrow(tt) set-49.csv'),verbose = FALSE)
test.file <- fread((input = '.../Desktop/data/MNIST-fashion testing set-49.csv'), verbose = FALSE)
```

```{r functions,echo=FALSE}
# Sampling function 
generating_samples <- function(size){
  the.rows <- sample(x = 1:train[, .N], size = size, replace = FALSE)
  dat <- train[the.rows,]
  return(dat)
}

# Error function
error <- function(label,preds){
  return(sum(label!=preds)/length(preds))
}

# Function that runs through the 9 iterations of a single model
Iteration <- function(FUN){
  dat = sample.sets
  size = vector()
  A = vector()
  B = vector()
  C = vector()
  Points = vector()
  names = unique(sample.sets.size.label)
  
  for(i in names){
    set = dat[sample.sets.size.label==i]
    size = c(size, nrow(set))
    
    run.model = FUN(set)
    A = c(A, run.model$score$A)
    B = c(B, run.model$score$B)
    C = c(C, run.model$score$C)
    Points = c(Points, run.model$score$Points)
  }
  
  result = data.table(`Sample Size` = size, Data = names, 
                      A = A, B = B, C = C, Points = Points)
  return(result)
}

# Function to compute the scoreboard results for a model at a given sample size
scoring <- function(rows, time, error){
  A <- round(rows/60000,4)
  B <- round(ifelse(time<60, time/60, 1),4)
  C <- round(error,4)
  Points = round(.25*A+.25*B+.5*C,4)
  
  return(list(A=A,B=B,C=C,
              Points=Points))
}
```

```{r constants,echo=FALSE}
n.values <- c(500, 1000, 2000)
iterations <- 3
```

```{r load_data,echo=FALSE}
train <- train.file
test <- test.file
```

```{r generate_samples,echo=FALSE}
# Generating 9 model develpment sets
for(n in n.values){
  for(k in 1:3){
    assign(paste("dat_",n,"_",k, sep=""),generating_samples(n))
  }
}

# Combining these 9 sets into a whole data set
sample.sets.raw <- rbind(dat_500_1, dat_500_2, dat_500_3,
                     dat_1000_1, dat_1000_2, dat_1000_3,
                     dat_2000_1, dat_2000_2, dat_2000_3)
sample.sets.size.label <- c(rep("500_1",500), rep("500_2",500), rep("500_3",500),
                            rep("1000_1",1000), rep("1000_2",1000), rep("1000_3",1000),
                            rep("2000_1",2000), rep("2000_2",2000),rep("2000_3",2000))
sample.sets <- cbind(sample.sets.raw, sample.sets.size.label)
# The first column of sample.sets is the label, and the last column of sample.sets is the dataset that a row belongs to
```



### Model 1:  Neural Networks

Neural network has a very complicated model structure. It digests data's features in a way like the brain of human beings. By fitting a neural network model to the data, it can grasp every possible feature that may influence the data labels and predict accurately.  

A neural network consists of weight matrix, activation function, and neurons which make up hidden layers. The input is data matrix multiplied by a weight matrix, and the weighted data goes into neurons. Filtered by an activation function, which decides whether weighted data can pass or not, the new data repeats the former steps again and again (depending on the layer number of the network) with different weight matrix and finally to the output. Loss function is computed by comparing the output with the true label, and taking derivation of loss function can obtain the optimization direction of the weights.  

The "size" parameter is the number of units in the hidden layer. On one hand, the complexity of the neural network's structure increases as number of units increases, and so does the computing time. On the other hand, increasing of units will enhance the prediction accuracy. It's important to find a trade-off between test error and computing time. I chose $size=10$ to decrease the computation time while ensure the accuracy of classification results. For the parameter "decay",it defines the decaying step of optimization procedure. The smaller decaying step, the less likely weights miss their optimized values, so I set $decay=5\times10^{-4}$. $Maxit$ parameter is the total number of rounds of the optimization procedure, and accuracy increases as rounds increase, but so does computing time. I set the it to be $10^5$.  

**Model building code**

```{r code_model1_development, eval = TRUE}
model1 <- function(dat){

  dat.label = class.ind(dat[,label])
  train = dat[,2:50] #note only the 2nd to 50th columns of data are trainable datapoints
  toc <- Sys.time() #time 1
  #fitting model
  model <- nnet(train, dat.label, size = 10, rang = 1/max(train),
                decay = 5e-4, maxit = 10^5, trace = FALSE)
  #predicting
  class.name <- function(vector){
    names(which.max(vector)) #turn the class matrix into class names
  } 
  train.preds <- apply(predict(model, train), 1, class.name)
  test.preds <- apply(predict(model, test[,-1]), 1, class.name)
  
  tic <- Sys.time() #time 2
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  #test error
  test.error <- error(test[,label], test.preds)
  
  #caculating score
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = test.error),
              train.preds = train.preds, test.preds=test.preds))
}
```

```{r load_model1,echo=FALSE}
model1.result <- Iteration(FUN = model1)
#datatable(model1.result)
```

Neural network models'complex model structure ensures their ability to dive into small details of data. However, it highly increases the risk of overfitting as well as time consuming. Thus, it's a model that is more suitable for very complicated datasets-say,natural language datasets. In our midterm project, the data is kind of too simple and complexity of model is not a must. Thus, the results are not as satisfied. The computing time is relatively long and the test errors are relatively high(larger than 0.3).


### Model 2:  Multinomial Logistic Regression

Multinomial logistic Regression is a generalized version of linear regression - it maps the classification problem into multiclass space, which is suitable for our image recognition problem, as we have 10 types of images. It's based on linear models $\boldsymbol{\beta}^T\boldsymbol{x}$. For each class k, we can obtain a coefficient vector $\boldsymbol{\beta}^T_k$, such that 
$$P(y=k|\boldsymbol{x})=\frac{e^{\boldsymbol{\beta_i^T}x}}{1+\sum_{i=1}^{k-1}e^{\boldsymbol{\beta_i^Tx}}}$$ and the k with highest probability is the predicted class.  

There are no specific parameters to select by running "multinom"" function. Just regress the label on each pixel and it automatically gives the optimized coefficients as well as classifications.  

**Model building code**

```{r code_model2_development,eval = TRUE}
model2 <- function(dat){

  train = dat[,1:50]
  toc <- Sys.time() #time 1
  #fitting model
  model <- multinom(label~., data=train, trace = FALSE)
  #predicting
  train.preds <- as.vector(predict(model, train[,-1]))
  test.preds <- as.vector(predict(model, test[,-1]))
  
  tic <- Sys.time() #time 2
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  #test error
  test.error <- error(test[,label], test.preds)
  
  #caculating score
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = test.error),
              train.preds = train.preds, test.preds = test.preds))
}
```

```{r load_model2,echo=FALSE}
model2.result <- Iteration(FUN = model2)
#datatable(model2.result)
```

It's basically a probability model so that it's easy to explain and apply. However, the model is sensitive to predictor variables that are highly correlated. In image recognition problem, we cannot guarantee the independence between each predictor(pixels), so the classifying is not as satisfied. The test error is relatively high(around 0.3).  

### Model 3 Ridge Regression

Basic linear regression uses least square estimation to obtain regression coefficients. However, when predictors in original dataset are highly multicollinearity related, the least square method will lose its advantage and produces extremely large coefficients, so that sensitive to noises. Ridge regression fixes this problem. (So does LASSO regression)  

Ridge regression adds a penalization term to the least square. It is $\lambda||w||^2_2$, and by minimizing the sum MSE and $\lambda||w||^2_2$, we guarantee that $w$ coefficients wouldn’t become too large. Using $glmnet$ function in R and set the argument $alpha$ to be 0, we get ridge regression models.  

Ridge regression effectively decreases the uncertainty of model by controlling the value of beta. However, it only reduces the value of beta rather than eliminating these coefficients, which makes the amount of predictor variables too large for a model.  

**Model building code**

```{r code_model3_development,eval = TRUE}
model3 <- function(dat){
  
  dat.label = dat[,label]
  train = as.matrix(dat[,2:50])
  toc <- Sys.time() #starting time
  
  #fitting model
  model <- glmnet(train, dat.label, family = "multinomial", alpha = 0)
  
  #predicting
  prob.train <- predict(model, train, 
                     type = "response", s = 0.01)[,,1]
  prob.test <- predict(model, as.matrix(test[,2:50]), 
                     type = "response", s = 0.01)[,,1]
  tic <- Sys.time() #ending time
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  #error
  train.preds <- colnames(prob.train)[apply(prob.train, 1, which.max)]
  test.preds <- colnames(prob.test)[apply(prob.test, 1, which.max)]
  test.error.mllr<- error(test[,1], preds = test.preds)
  
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = test.error.mllr),
         train.preds = train.preds, test.preds = test.preds))
        
}
```

```{r load_model3,echo=FALSE}
model3.result <- Iteration(model3)
#datatable(model3.result)
```


### Model 4 LASSO Regression

LASSO stands for Least Absolute shrinkage and selection operator and it is a relatively recent alternative to RIDGE regression. It minimizes the quantity that the penity term becomes $\lambda \sum|\beta_j|$ here. We picked up LASSO as Ridge Regression has one obvious disadvantage that it concludes all the predictors in our final model, and the penalty term $\lambda \sum\beta_j^2$ will shrink all of the coefficients towards zero, not equal to zero. In this way, it will be difficult to interprate the settings in terms of the number of variables, and this is the reason why we choose LASSO to solve this problem.  

LASSO regression is one type of linear regression that we can use the Shrinkage as a tool. In the other words, LASSO prefer to choose simple models with fewer parameters compared to RIDGE regression.  

Using `glmnet` function in R and set the argument $alpha$ to be 1, actually the default for the glmnet function is $\alpha$=1, and then we can get the LASSO regression model being fit to our data.  

LASSO provides a good prediction accuracy as it removes the coefficients and decreases variance without a substantial increase of the bias. It is useful when we do not have enough observations and we need to predict a lot of features. It has good model interpretability by eliminating irrelated variables and it reduces the chance of overfitting. However, LASSO's penalty is too large when the error is small (close to 0).  

**Model building code**

```{r code_model4_development,eval = TRUE}
model4 <- function(dat){
  
  dat.label = dat[,label]
  train = as.matrix(dat[,2:50])
  toc <- Sys.time() #starting time
  
  #fitting model
  lasso.lr<- glmnet(train, dat.label, family = "multinomial")
  
  #predicting
  prob.train <- predict(lasso.lr, train,
                        type = "response", s = 0.01)[,,1]
  prob.test <- predict(lasso.lr, as.matrix(test[,2:50]), 
                     type = "response", s = 0.01)[,,1]
  tic <- Sys.time() #ending time
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  #error
  train.preds <- colnames(prob.train)[apply(prob.train, 1, which.max)]
  test.preds <- colnames(prob.test)[apply(prob.test, 1, which.max)]
  test.error.mllr<- error(test[,1], preds = test.preds)
  
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = test.error.mllr),
              train.preds = train.preds, test.preds = test.preds))
  
}
```

```{r load_model4,echo=FALSE}
model4.result <- Iteration(FUN = model4)
#datatable(model4.result)
```

LASSO is generated successfully, and we can see that it shows a good prediction accuracy in terms of Point function.

### Model 5 K Nearest Neighbours with K=10

The k-nearest neighbors algorithm (KNN) is a non-parametric method which can be used for both classification and regression. We chose it as our model because it is commonly used for its ease of interpretation and low calculation time. We predict the data points based on the average of the outcomes of the K nearest neighbors.  

KNN based on distance metric such as Euclidean distance to determine what constitutes the K nearest neighbors. 
If k is big enough, we will be able to cover the whole training set, and it becomes naive Bayesian model, so it will influence the accuracy. If k is too small, the decision boundary will become unstable. One small change in the training set may cause the big change in our classification results.

The advantage of the model is that it is easy to understand and has a developed system when applied to classification and regression. Besides, it is not sensitive to outliers. However, the calculation process is difficult and the accuracy could not be ensured, and it cannot interpret the meanings of the data.

When considering the choice of K value, we can use cross-validation, which could give us the result of a good choice of k. The following code shows how we get the optimal k procedure.

```{r, eval=FALSE}
cross.validation<-function(dat){
ctrl <- trainControl(method="repeatedcv",repeats = 3)
knnFit <- train(label ~ ., data = dat, method = "knn", trControl = ctrl, preProcess = c("center","scale"),tuneLength = 20)
return(knnFit)
}
```

We are choosing K=10 here to have a rough idea about KNN process.

**Model building code**

```{r code_model5_development,eval = TRUE}
model5 <- function(dat){
  
  dat.label = as.matrix(dat[,1])
  train = dat[,2:50]
  
  toc <- Sys.time() #starting time
  
  #fitting model and predicting
  train.preds <- knn(train, train, 
                     cl=as.factor(as.matrix(dat[,1])), k=10)
  test.preds <- knn(train, test[,2:50], 
                    cl=as.factor(as.matrix(dat[,1])), k=10)
  
  tic <- Sys.time() #ending time
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  #error
  test.error.knn<- error(test[,1], preds = as.matrix(test.preds))
  
  return(list(score=scoring(rows = nrow(dat), time = the.time, error = test.error.knn),
              train.preds=as.vector(train.preds), 
              test.preds=as.vector(test.preds)))
  
}
```

```{r load_model5,echo=FALSE}
model5.result <- Iteration(FUN = model5)
# datatable(model5.result)
```

It is easy to see that the running time of KNN model is shorter compared to the others. Points are around 0.12 t0 0.15.

### Model 6 K Nearest Neighbours with K=5

Now we change the parameter K from K=10 to K=5. As a matter of fact, after trying several values of our parameter K, we found that K=5 is the optimal choice in terms of Point function.

**Model building code**

```{r code_model6_development,eval = TRUE}
model6<- function(dat){
  
  dat.label = as.matrix(dat[,1])
  train = dat[,2:50]
  toc <- Sys.time() #starting time
  
  #fitting model and predicting
  train.preds <- knn(train, train,
                     cl=as.factor(as.matrix(dat[,1])), k=5)
  test.preds <- knn(train, test[,2:50], 
                    cl=as.factor(as.matrix(dat[,1])), k=5)
  
  tic <- Sys.time() #ending time
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  #error
  test.error.knn<- error(test[,1], preds = as.vector(test.preds))
  
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = test.error.knn),
              train.preds = as.vector(train.preds),
              test.preds = as.vector(test.preds)))
  
}
```

```{r load_model6,echo=FALSE}
model6.result <- Iteration(model6)
# datatable(model6.result)
```

### Model 7 Random Forest- Ntree = 500 and mtry = 7

Random Forest usually provides higher accuracy than most other models. In random forest, a number of decision trees are built from bootstrapped training samples i.e. random samples that are selected from the training set with replacement. While building the decision trees, each time a split in tree is considered, a random set of m predictors is selected from a full set of p predictors. The split is allowed to use only one of those m predictors. The number of predictors considered at each split is usually equal to the square root of the total number of predictors.  

For the parameters, we decided to go with 500 trees since it reduces the computational burden in comparison to selecting a larger number of trees but at the same time it also decreases the variance . For the `Mtry` parameter, since we have 49 variables and 7 is the square root of 49, 7 is the default number of variables randomly sampled.  
Random forest de-correlates the trees. This reduces variance and improves the accuracy of the model. It can be used for classification and regression models. However, this brings greater computational complexity.  

**Model building code**

```{r code_model7_development,eval = TRUE}
model7 <- function(dat){
  dat$label <- as.factor(dat$label)
  train = dat[,1:50]
  
  #time
  toc <- Sys.time() #time begin
  
  model <- randomForest(label~., data = train, ntree = 500, mtry = 7)
  
  train.preds <- as.vector(predict(model, newdata = train))
  test.preds <- as.vector(predict(model, newdata = test))
  
  tic <- Sys.time() #time finish
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  error <- error(test[,label], test.preds)
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = error),
              train.preds = train.preds, test.preds = test.preds))
}
```

```{r load_model7,echo=FALSE}
model7.result <- Iteration(FUN = model7)
#datatable(model7.result)
```

### Model 8 Random Forest - Ntree = 1000

We increased the number of trees to 1000 so see how it would affect the model and I noticed that while the accuracy was similar for both the models, the model with 1000 trees took more time to run. After analysing the points of both the models and comparing the accuracy and computing time, I believe that model 7 is a better model that model 8. 

```{r code_model8_development,eval = TRUE}
model8 <- function(dat){

  train = dat[,1:50]
  
  #time
  toc <- Sys.time() #time begin
  
  model <- randomForest(as.factor(label)~., data = train, ntree = 1000, mtry = 7)
  
  train.preds <- as.vector(predict(model, newdata = train))
  test.preds <- as.vector(predict(model, newdata = test))
  
  tic <- Sys.time() #time finish
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  error <- error(test$label, test.preds)
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = error),
              train.preds = train.preds, test.preds = test.preds))
}
```

```{r load_model8,echo=FALSE}
model8.result <- Iteration(FUN = model8)
#datatable(model8.result)
```

### Model 9 Classification tree

Classification tree is a method that is easy to interpret and it can be used to predict categorical outcomes. At the top of the tree the best variable is selected, then the variable is split into two or more branches. The branching continues until the best stopping criteria has reached.  

When building model, we selected `Method= “class”` since classification trees can be used to predict qualitative outcomes.  

A classification tree is easy to interpret and understand the data. Besides, this model can easily be explained to others since categorical splits are easy to understand. However, a tree can become complex if it is too large, and their level of accuracy is not as good as the other models.  

**Model building code**

```{r code_model9_development,eval = TRUE}
model9 <- function(dat){
  train = dat[,1:50]
 
   #time
  toc <- Sys.time() #time begin
  
  model <- rpart(as.factor(label)~.,method="class", data=train)
  
  train.preds <- as.vector(predict(model, newdata = train, type = "class"))
  test.preds <- as.vector(predict(model, newdata = test, type = "class"))
  
  tic <- Sys.time() #time finish
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  error <- error(test$label, test.preds)
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = error),
              train.preds = train.preds, test.preds = test.preds))
}
```

```{r load_model9,echo=FALSE}
model9.result <- Iteration(model9)
#datatable(model9.result)
```

### Model 10 Ensemble Model

We decided to use build an ensemble model to improve the accuracy of our prediction. Constructing such a model helps with understanding how the predictions of our other models could be combined together to improve the accuracy.  

Utilizing predictions of training sets from the other models as predictor and the true labels of training sets as response vector, we trained a random forest model to obtain reasonable weights for each foundation model. Having obtained the optimized coefficients of this ensemble model, we use the classification results of test sets from the 8 foundation models as input to obtain the outcome of the test set.  

We selected to use the predictions of model 2 - 8 to build an ensemble model to predict the outcome. We decided to exclude model 1 (neural network model) since its running time is very high.  

The advantage of this model is obvious - it helps in improving machine learning models by using the predictions of other models such that better predictions in comparison to single models. However, this requires greater computational complexity, and it's hard to decide which based models should be used exactly to construct this essemble model.  

**Model building code**

```{r code_model10_development,eval = TRUE}
model10 <- function(dat){

  toc <- Sys.time() #time begin
  
  # Constructing model
  
  results <- list()
  # Creating a list to store each model's results
  results[[1]] <- model2(dat)
  results[[2]] <- model3(dat)
  results[[3]] <- model4(dat)
  results[[4]] <- model5(dat)
  results[[5]] <- model6(dat)
  results[[6]] <- model7(dat)
  results[[7]] <- model8(dat)
  results[[8]] <- model9(dat)
  
  # In this model, we use model 2 to model 9 to construct the essemble model
  
  # Creating a matrix whose columns are predictions of training sets computed by each model
  # with the first column as the true label of training set
  train.preds.matrix <- matrix(nrow = nrow(dat), ncol = 9)
  colnames(train.preds.matrix) <- c("label", paste("m",2:9, sep = ""))
  train.preds.matrix[,1] <- dat[,label]
  for(i in 2:9){
    train.preds.matrix[,i] <- results[[i-1]]$train.preds
  }
  
  train.preds.matrix <- apply(train.preds.matrix, 2, as.factor)

  # Fit a randomforest model to classes of training set classified by each model 
  #against true training labels 
  model <- randomForest(label~., train.preds.matrix)
  
  # Creating a matrix whose columns are predictions of test sets computed by each model
  test.preds.matrix <- matrix(nrow = nrow(test), ncol = 8)
  colnames(test.preds.matrix) <- paste("m",2:9, sep = "")

  for(i in 1:8){
    test.preds.matrix[,i] <- results[[i]]$test.preds
  }
  
  test.preds.matrix <- apply(test.preds.matrix, 2, as.factor)
  
  # Make predictions by inputing the predictions of test set produced by each model
  test.preds <- as.vector(predict(model, newdata = test.preds.matrix))
  
  tic <- Sys.time() #time finish
  the.time <- as.numeric(x = tic-toc, units = "secs")
  
  # Computing error
  error = error(test[,label], test.preds)
  
  return(list(score = scoring(rows = nrow(dat), time = the.time, error = error),
              test.preds = test.preds))
}
```

```{r load_model10,echo=FALSE}
model10.result <- Iteration(model10)
#datatable(model10.result)
```

### Scoreboard

The total scoreboard of our model is shown as following (codes of editing scoreboard are hidden). For each sample size of each model, we take the average of the points and display the table in an increasing order of points. A is the proportion of training rows used, B is the computing time, and C is the test error. 

$$Points = 0.25A+0.25B+0.5C$$

```{r scoreboard,eval = TRUE, echo=FALSE}
avg.score <- function(result = results){
  A = vector()
  B = vector()
  C = vector()
  Points = vector()
  for(i in 0:2){
    index = (i*3+1):(i*3+3) #index of the rows to be averaged:1-3,4-6,7-9
    A = c(A, mean(result[index,A]))
    B = c(B, mean(result[index,B]))
    C = c(C, mean(result[index,C]))
    Points = c(Points, mean(result[index,Points]))
  }
  return(data.table(`Sample Size` = n.values,
                    A = round(A,4), B = round(B,4), C = round(C,4), 
                    Points = round(Points,4)))
}

scoreboard <- rbind(avg.score(model1.result), avg.score(model2.result), 
                    avg.score(model3.result), avg.score(model4.result),
                    avg.score(model5.result), avg.score(model6.result),
                    avg.score(model7.result), avg.score(model8.result),
                    avg.score(model9.result), avg.score(model10.result))
scoreboard <- cbind(Model=c(rep(1,3),rep(2,3),rep(3,3),rep(4,3),rep(5,3),
                       rep(6,3),rep(7,3),rep(8,3),rep(9,3),rep(10,3)), scoreboard)
setorderv(scoreboard, cols = "Points", order = 1)
datatable(scoreboard)
```

## References

* Goswami, A., & Goswami, A. (2018, February 08). Intro to image classification with KNN. Retrieved from https://medium.com/@YearsOfNoLight/intro-to-image-classification-with-knn-987bc112f0c2

* En.wikipedia.org. (2019). Multinomial logistic regression. [online] Available at:https://en.wikipedia.org/wiki/Multinomial_logistic_regression [Accessed 13 Mar. 2019].

* Chakon, O. (2019). Practical machine learning: Ridge regression vs. Lasso - Coding Startups. [online] Coding Startups. Available at: https://codingstartups.com/practical-machine-learning-ridge-regression-vs-lasso/ [Accessed 13 Mar. 2019].

* Donges, N., & Donges, N. (2018, February 22). The Random Forest Algorithm. Retrieved from https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd

* \10105510377845702. (2017, August 22). Ensemble Learning to Improve Machine Learning Results. Retrieved from https://blog.statsbot.co/ensemble-learning-d1dcd548e936

* Imran. (2015, November 15). How to choose the value of K in knn algorithm. Retrieved from https://discuss.analyticsvidhya.com/t/how-to-choose-the-value-of-k-in-knn-algorithm/2606/3

* Reffered to the lecture notes of columbia applied data science course

